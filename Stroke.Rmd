---
title: "Stroke Dataset"
author: "Hunter Blum, Ben Earnest, Andrew Pak Kim"
date: "3/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Dataset:

https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset

# Libraries
```{r}
library(tidyverse)
library(caret)
library(NeuralNetTools)
library(neuralnet)
library(nnet)
library(smotefamily)
library(unbalanced)
library(doParallel)

set.seed(123)
```

# Data
```{r}
Stroke <- read.csv("stroke.csv")
```

# Multicore Support - Need to have Java Installed that is the same bit as your CPU (Probably 64)
```{r}
registerDoParallel()
getDoParWorkers()
```

# Cleaning the Data

The data is pretty clean after running this code. All variables are the correct type after running it. The only variable missing data is BMI, which only has 200/5100 observations missing. 

## Structure
```{r}
str(Stroke)

#Get rid of one other observation in gender
Stroke <- Stroke %>% filter(gender!="Other")

#Fix specific variables
Stroke$hypertension <- as.factor(Stroke$hypertension)
Stroke$heart_disease <- as.factor(Stroke$heart_disease)
Stroke$bmi <- as.numeric(Stroke$bmi)
Stroke$stroke <- as.factor(Stroke$stroke)

#Make all character variables into factors
Stroke[sapply(Stroke, is.character)] <- lapply(Stroke[sapply(Stroke, is.character)], as.factor)


str(Stroke)

#Rename Factors for Easier Understanding
levels(Stroke$hypertension) <- c("No", "Yes")
levels(Stroke$heart_disease) <- c("No", "Yes")
levels(Stroke$stroke) <- c("No", "Yes")

#Get Rid of id
Stroke$id <- NULL
```
## NAs
```{r}
Stroke %>% 
  select(everything()) %>% 
  summarise_all(funs(sum(is.na(.))))
#Just 201 missing observations in bmi 

#We'll just delete the NAs for now
Stroke_clean <- na.omit(Stroke)
```



# Exploratory Data Analysis

Our target feature is stroke, where 1 indicates that a stroke occurred. For any binary attributes 1 is always the variable occurred (eg. 1 for heart disease means the patient had heart disease). 

## Dataset Overview
```{r}
summary(Stroke_clean)
head(Stroke_clean)
```


## Variable by Stroke
### Make Functions
```{r}
#Categorical
Cat_eda <- function(x, y) {
  p1 <- ggplot(Stroke_clean, aes(x={{x}})) +
    geom_bar(aes(fill=stroke) , color = "black") +
    ggtitle(paste0("Stroke with Respect to ", y)) +
    xlab(y) + ylab("Count")

  p2 <- ggplot(Stroke_clean, aes(x={{x}})) +
    geom_bar(aes(fill=stroke), position = "fill", color = "black") + ggtitle(paste0("Stroke with Respect to ",y, " (Normalized)")) + xlab(y) + ylab("Count")
  
  plot(p1)
  plot(p2)
}

#Numeric
Num_eda <- function(x, y) {
  p1 <- ggplot(Stroke_clean, aes(x={{x}})) +
    geom_histogram(aes(fill=stroke), color = "black") +
    ggtitle(paste0("Stroke with Respect to ", y)) +
    xlab(y) + ylab("Count")

 p2 <- ggplot(Stroke_clean, aes(x={{x}})) +  
   geom_histogram(aes(fill=stroke), color = "black", position =      "fill") +
   ggtitle(paste0("Stroke with Respect to ", y)) +
   xlab(y) + ylab("Count")
 
 plot(p1)
 plot(p2)
  
}

```


### Categorical Variables
```{r}
Cat_eda(gender, "Gender")
Cat_eda(hypertension, "Hypertension")
Cat_eda(heart_disease, "Heart Disease")
Cat_eda(ever_married, "Ever Married")
Cat_eda(work_type, "Work Type")
Cat_eda(Residence_type, "Residence Type")
Cat_eda(smoking_status, "Smoking Status")
``` 

### Numeric variables
```{r}
Num_eda(age, "Age")
Num_eda(avg_glucose_level, "Avg. Glucose Level")
Num_eda(bmi, "BMI")
```


# Data Preparation for modelling

## Partition Data 
```{r}
#Partition Data
trainIndex <- createDataPartition(y=Stroke_clean$stroke, p=0.8, list = F, times = 1)

Stroke_tr <- Stroke_clean[trainIndex, ]
Stroke_test <- Stroke_clean[-trainIndex, ]
```

## Balance Data?

### Visualize Stroke Balance in Training 
```{r}
#Dual Axis
ggplot(Stroke_tr, aes(x=stroke)) + 
  geom_bar() +
  scale_y_continuous(
    name = "Count",
    sec.axis = sec_axis(~./nrow(Stroke_tr), name = "Proportion")
  ) + ggtitle("Training Data")
```

### Oversampling
```{r}
#Get count of yes in training
minority <- Stroke_tr %>% group_by(stroke) %>% tally() %>% filter(stroke =="Yes")

#Change this to change balance to desired yes proportion
increase_to <- 0.5

#Calculate resample amouunt
oversample_n <- (increase_to*nrow(Stroke_tr)-minority$n)/(1-increase_to)

#Resample
to_oversample <- which(Stroke_tr$stroke == "Yes")
our_oversample <- sample(x = to_oversample, size = oversample_n, replace = T)
our_oversample <- Stroke_tr[our_oversample, ]
Stroke_over <- rbind(Stroke_tr, our_oversample)

#Evaluate
ggplot(Stroke_over, aes(x=stroke)) + 
  geom_bar() +
  scale_y_continuous(
    name = "Count",
    sec.axis = sec_axis(~./nrow(Stroke_over), name = "Proportion")
  )  + ggtitle("Oversampled Data")
```

### SMOTE?

## Standardization
### Min-Max Standardization Function - Use standard.df() to create your own data set for model if you feel standardization is necessary.
```{r}
#Function to Standardize One Variable
standard.mm <- function(x){
  (x - min(x)) / (max(x) - min(x))
}

#Function to Standardize all Numeric Variables in Data Frame
standard.mm.df <- function(x){
  #Split Data
  tr_num <- x %>% select(where(is.numeric))
  tr_non <- x %>% select(!where(is.numeric))
  
  #Run Standardization Function Across Numeric
  tr_num_mm <- apply(X = tr_num, FUN = standard.mm, MARGIN = 2)
  
  #Recombine
  tr_mm <- cbind(tr_non, tr_num_mm)
}

```

### Z-Score Standardization
```{r}
#Z-Score Function
standard.z <- function(x){
  (x-mean(x))/sd(x)
}

#Function to Standardize all Numeric Variables in Data Frame
standard.z.df <- function(x){
  #Split Data
  tr_num <- x %>% select(where(is.numeric))
  tr_non <- x %>% select(!where(is.numeric))
  
  #Run Standardization Function Across Numeric
  tr_num_mm <- apply(X = tr_num, FUN = standard.z, MARGIN = 2)
  
  #Recombine
  tr_mm <- cbind(tr_non, tr_num_mm)
}

```

# Modeling
## C5.0 - Andrew
```{r}
library(C50)
C5 <- C5.0(formula = stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data = Stroke_over, control = C5.0Control(minCases = 75))
```
```{r}
#Visualize the tree
plot(C5)
```

```
{r}
```
#Obtain the classifications for each record in the data set. 
X = data.frame(gender = Stroke_over$gender, age = Stroke_over$age, hypertension = Stroke_over$hypertension, heart_disease = Stroke_over$heart_disease, ever_married = Stroke_over$ever_married, work_type = Stroke_over$work_type, Residence_type = Stroke_over$Residence_type, avg_glucose_level = Stroke_over$avg_glucose_level, bmi = Stroke_over$bmi, smoking_status = Stroke_over$smoking_status)
Stroke_over_pred <- predict(object = cart01, newdata = X, type = "class")
predict(object = C5, newdata = X)
```


## CART - Ben

###Install rpart and rpart.plot, run CART decision tree model:
```{r}
library(rpart)
library(rpart.plot)
cart01 <- rpart(formula = stroke ~ gender + age + hypertension + heart_disease + ever_married + work_type + Residence_type + avg_glucose_level + bmi + smoking_status, data = Stroke_over, method = "class")
rpart.plot(cart01)
```
###obtain the predicted responses:

```{r}
X = data.frame(gender = Stroke_over$gender, age = Stroke_over$age, hypertension = Stroke_over$hypertension, heart_disease = Stroke_over$heart_disease, ever_married = Stroke_over$ever_married, work_type = Stroke_over$work_type, Residence_type = Stroke_over$Residence_type, avg_glucose_level = Stroke_over$avg_glucose_level, bmi = Stroke_over$bmi, smoking_status = Stroke_over$smoking_status)
Stroke_over_pred <- predict(object = cart01, newdata = X, type = "class")

```

###Create Confusion matrix for the training data set "Stroke_over" actual vs predicted results:

```{r}
Stroke_over$stroke_pred <- Stroke_over_pred
cart01_t1 <- table(Stroke_over$stroke, Stroke_over$stroke_pred)
row.names(cart01_t1) <- c("Actual: No", "Actual: Yes")
colnames(cart01_t1) <- c("Predicted: No", "Predicted: Yes")
cart01_t1 <- addmargins(A = cart01_t1, FUN = list(Total = sum), quiet = TRUE)
cart01_t1.df <- as.data.frame.matrix(cart01_t1)
cart01_t1.df

```
###Evaluate model performance on training data:

```{r}
#calculate evaluation metrics
precision_1 <- cart01_t1.df[2,2] / cart01_t1.df[3,2]
Accuracy_1 <- (cart01_t1.df[1,1] +cart01_t1.df[2,2])/cart01_t1.df[3,3]
error_rate_1 <- 1 - Accuracy_1
sensitivity_1 <- cart01_t1.df[2,2]/cart01_t1.df[1,3]
specificity_1 <- cart01_t1.df[1,1]/cart01_t1.df[1,3]
f1_1 <- 2*((precision_1 * specificity_1)/(precision_1+specificity_1))
f2_1 <- 5*((precision_1*specificity_1)/((4*precision_1)+specificity_1))
f0.5_1 <- 1.25*((precision_1 * specificity_1)/((0.25*precision_1)*specificity_1))
#create dataframe for evaluation metrics
eval.df1 <- data.frame(eval.measure = c("Accuracy", "error.rate", "sensitivity", "specificity", "precision", "f1", "f2", "f0.5"))
eval.df1$model.1 <- round(c(Accuracy_1, error_rate_1, sensitivity_1, specificity_1, precision_1, f1_1, f2_1, f0.5_1), 2)
eval.df1

```
###Now run the test data through the model and evaluate performance.

```{r}
#store variables to pass through model in dataframe:
X = data.frame(gender = Stroke_test$gender, age = Stroke_test$age, hypertension = Stroke_test$hypertension, heart_disease = Stroke_test$heart_disease, ever_married = Stroke_test$ever_married, work_type = Stroke_test$work_type, Residence_type = Stroke_test$Residence_type, avg_glucose_level = Stroke_test$avg_glucose_level, bmi = Stroke_test$bmi, smoking_status = Stroke_test$smoking_status)
#run test dataset through trained model:
Stroke_test_pred <- predict(object = cart01, newdata = X, type = "class")
#create confustion matrix dataframe:
Stroke_test$stroke_pred <- Stroke_test_pred
cart01_t2 <- table(Stroke_test$stroke, Stroke_test$stroke_pred)
row.names(cart01_t2) <- c("Actual: No", "Actual: Yes")
colnames(cart01_t2) <- c("Predicted: No", "Predicted: Yes")
cart01_t2 <- addmargins(A = cart01_t2, FUN = list(Total = sum), quiet = TRUE)
cart01_t2.df <- as.data.frame.matrix(cart01_t2)
cart01_t2.df
```
###Evaluate model performance on training data:

```{r}
#calculate evaluation metrics
precision_2 <- cart01_t2.df[2,2] / cart01_t2.df[3,2]
Accuracy_2 <- (cart01_t2.df[1,1] +cart01_t2.df[2,2])/cart01_t2.df[3,3]
error_rate_2 <- 1 - Accuracy_1
sensitivity_2 <- cart01_t2.df[2,2]/cart01_t2.df[1,3]
specificity_2 <- cart01_t2.df[1,1]/cart01_t2.df[1,3]
f1_2 <- 2*((precision_2 * specificity_2)/(precision_2+specificity_2))
f2_2 <- 5*((precision_2*specificity_2)/((4*precision_2)+specificity_2))
f0.5_2 <- 1.25*((precision_2 * specificity_2)/((0.25*precision_2)*specificity_2))
#create dataframe for evaluation metrics
eval.df2 <- data.frame(eval.measure = c("Accuracy", "error.rate", "sensitivity", "specificity", "precision", "f1", "f2", "f0.5"))
eval.df2$model.2 <- round(c(Accuracy_2, error_rate_2, sensitivity_2, specificity_2, precision_2, f1_2, f2_2, f0.5_2), 2)
eval.df2
```



## Logistic Regression - Ben



## Random Forest - Hunter
```{r}
rf_stroke <- caret::train(stroke~., method="rf", data = Stroke_tr, tuneLength = 5, trControl = trainControl(
  method = "cv", indexOut = train
))

rf_stroke
```


## Naive Bayes - Andrew
```{r}

```

## Association Rule - Ben




## Neural Network - Hunter

### Fitting Models
```{r}
#Unbalanced
train <- createFolds(Stroke_tr$stroke, k=10)

nnet_stroke <- caret::train(stroke ~ ., method = "nnet", data = Stroke_tr,
    tuneLength = 5,
    trControl = trainControl(
        method = "cv", indexOut = train),
  trace = FALSE)   
                     
                     

nnet_stroke
plotnet(nnet_stroke$finalModel)

#Balanced

train <- createFolds(Stroke_over$stroke, k=10)

nnet_stroke_balanced <- caret::train(stroke ~ ., method = "nnet", data = Stroke_over,
    tuneLength = 5,
    trControl = trainControl(
        method = "cv", indexOut = train),
  trace = FALSE)   
   
nnet_stroke_balanced
plotnet(nnet_stroke_balanced$finalModel)

#Z-Score Standardized

Stroke_tr_z_nnet <- standard.z.df(Stroke_tr)

train <- createFolds(Stroke_tr_z_nnet$stroke, k=10)


nnet_stroke_z <- caret::train(stroke ~ ., method = "nnet", data = Stroke_tr_z_nnet,
    tuneLength = 5,
    trControl = trainControl(
        method = "cv", indexOut = train),
  trace = FALSE)   
   
nnet_stroke_z
plotnet(nnet_stroke_z$finalModel)

#Z-score standardized and balanced

Stroke_tr_z_bal <- standard.z.df(Stroke_over)

train <- createFolds(Stroke_tr_z_nnet$stroke, k=10)


nnet_stroke_z_bal <- caret::train(stroke ~ ., method = "nnet", data = Stroke_over,
    tuneLength = 5,
    trControl = trainControl(
        method = "cv", indexOut = train),
  trace = FALSE)   
   
nnet_stroke_z_bal
plotnet(nnet_stroke_z_bal$finalModel)
```

### Evaluate NN
```{r}
confusionMatrix(data = predict(nnet_stroke, Stroke_test), ref = Stroke_test$stroke, positive = "Yes")

confusionMatrix(data = predict(nnet_stroke_balanced, Stroke_test), ref = Stroke_test$stroke, positive = "Yes")

confusionMatrix(data = predict(nnet_stroke_z, Stroke_test), ref = Stroke_test$stroke, positive = "Yes")

confusionMatrix(data = predict(nnet_stroke_z_bal, Stroke_test), ref = Stroke_test$stroke, positive = "Yes")
```

# Model Evaluation
```{r}

```
